{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP38tNzhh2aXu5idZiliZdk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanyijohn/Deep-Learning-ML-models/blob/main/LLM_Classification_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f0f2dbd"
      },
      "source": [
        "# Task\n",
        "Explore using pre-trained language models (like BERT, RoBERTa, or others) to generate embeddings for the prompt and responses with More Complex Model Architecture to Consider more sophisticated neural network architectures designed for sequence comparison or ranking. You could fine-tune these models on the preference prediction task. Systematically tune hyperparameters like embedding dimension, maxlen, number of layers, units in dense layers, learning rate, batch size, and number of epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81beeb2b"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the `transformers` library to use pre-trained language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84589c7"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the transformers library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28185765",
        "outputId": "c2884e6c-daa2-45f2-9c2d-edffd3bc2b6a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81047b09"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "### Subtask:\n",
        "Load pre-trained model and tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d73c6e0b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary classes and load the pre-trained tokenizer and model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4_sNOnEj_xU",
        "outputId": "bb857123-2f33-4393-a97d-161cde5abdcc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.9)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Upload the file again\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Verify the contents (make sure username & key are correct)\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Uploaded: {filename}\")\n",
        "    print(uploaded[filename].decode('utf-8'))  # Check the key is correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "ewne627NkOVc",
        "outputId": "3e934cb7-4ae7-46ee-fa7a-214a2fa2e768"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-969c1751-be08-419a-8c9d-cc629f1bf4b9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-969c1751-be08-419a-8c9d-cc629f1bf4b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Uploaded: kaggle.json\n",
            "{\"username\":\"johnsonkanyi\",\"key\":\"f8e3cd38e45c532ab64524da20ece09e\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle  # -p prevents error if dir exists\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Restrict permissions"
      ],
      "metadata": {
        "id": "vs06Ph8kkWOt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions list  # Should list competitions (no 401 error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tK3O4Tg0kazq",
        "outputId": "90ac2067-4d0d-4b42-c39f-9bd74b878df0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                                              deadline             category                reward  teamCount  userHasEntered  \n",
            "-------------------------------------------------------------------------------  -------------------  ---------------  -------------  ---------  --------------  \n",
            "https://www.kaggle.com/competitions/arc-prize-2025                               2025-11-03 23:59:00  Featured         1,000,000 Usd        657           False  \n",
            "https://www.kaggle.com/competitions/google-gemma-3n-hackathon                    2025-08-06 23:59:00  Featured           150,000 Usd          0           False  \n",
            "https://www.kaggle.com/competitions/make-data-count-finding-data-references      2025-09-09 23:59:00  Research           100,000 Usd        608           False  \n",
            "https://www.kaggle.com/competitions/map-charting-student-math-misunderstandings  2025-10-15 23:59:00  Featured            55,000 Usd        243           False  \n",
            "https://www.kaggle.com/competitions/cmi-detect-behavior-with-sensor-data         2025-09-02 23:59:00  Featured            50,000 Usd       1845           False  \n",
            "https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025         2025-09-15 23:59:00  Featured            50,000 Usd       1283           False  \n",
            "https://www.kaggle.com/competitions/ariel-data-challenge-2025                    2025-09-24 23:59:00  Featured            50,000 Usd        281           False  \n",
            "https://www.kaggle.com/competitions/meta-kaggle-hackathon                        2025-07-21 23:59:00  Featured            50,000 Usd          0           False  \n",
            "https://www.kaggle.com/competitions/playground-series-s5e7                       2025-07-31 23:59:00  Playground                Swag       2403           False  \n",
            "https://www.kaggle.com/competitions/titanic                                      2030-01-01 00:00:00  Getting Started      Knowledge      15501           False  \n",
            "https://www.kaggle.com/competitions/home-data-for-ml-course                      2030-01-01 23:59:00  Getting Started      Knowledge       5799           False  \n",
            "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques  2030-01-01 00:00:00  Getting Started      Knowledge       4735           False  \n",
            "https://www.kaggle.com/competitions/spaceship-titanic                            2030-01-01 00:00:00  Getting Started      Knowledge       2006           False  \n",
            "https://www.kaggle.com/competitions/digit-recognizer                             2030-01-01 00:00:00  Getting Started      Knowledge       1183           False  \n",
            "https://www.kaggle.com/competitions/nlp-getting-started                          2030-01-01 00:00:00  Getting Started      Knowledge        934            True  \n",
            "https://www.kaggle.com/competitions/store-sales-time-series-forecasting          2030-06-30 23:59:00  Getting Started      Knowledge        697           False  \n",
            "https://www.kaggle.com/competitions/llm-classification-finetuning                2030-07-01 23:59:00  Getting Started      Knowledge        338            True  \n",
            "https://www.kaggle.com/competitions/connectx                                     2030-01-01 00:00:00  Getting Started      Knowledge        203           False  \n",
            "https://www.kaggle.com/competitions/gan-getting-started                          2030-07-01 23:59:00  Getting Started      Knowledge        126           False  \n",
            "https://www.kaggle.com/competitions/contradictory-my-dear-watson                 2030-07-01 23:59:00  Getting Started      Knowledge         54           False  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c llm-classification-finetuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7MBbjoRke1p",
        "outputId": "bd9c8714-5577-4f2b-99f8-2c2a864d9819"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llm-classification-finetuning.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llm-classification-finetuning.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An_3bDrpklvG",
        "outputId": "9a661b35-444c-473a-8fa8-1bee929beaa6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llm-classification-finetuning.zip\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Setup and Data Loading\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, GlobalAveragePooling1D, concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re # For text cleaning\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure train.csv, test.csv, and sample_submission.csv are in your Colab environment.\")\n",
        "    # You might need to upload the files or connect to Google Drive\n",
        "    # from google.colab import files\n",
        "    # uploaded = files.upload()\n",
        "    # For Kaggle competitions, the data is usually available in the input directory\n",
        "    # train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\n",
        "    # test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n",
        "    # sample_submission_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/sample_submission.csv')\n",
        "\n",
        "print(\"\\nTrain data shape:\", train_df.shape)\n",
        "print(\"Test data shape:\", test_df.shape)\n",
        "print(\"\\nTrain data head:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nTest data head:\")\n",
        "print(test_df.head())\n",
        "print(\"\\nSample submission head:\")\n",
        "print(sample_submission_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIK4Id5TkrfK",
        "outputId": "2ec6a4fa-86c5-4074-81a1-dbb7a0523af3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "\n",
            "Train data shape: (57477, 9)\n",
            "Test data shape: (3, 4)\n",
            "\n",
            "Train data head:\n",
            "       id             model_a              model_b  \\\n",
            "0   30192  gpt-4-1106-preview           gpt-4-0613   \n",
            "1   53567           koala-13b           gpt-4-0613   \n",
            "2   65089  gpt-3.5-turbo-0613       mistral-medium   \n",
            "3   96401    llama-2-13b-chat  mistral-7b-instruct   \n",
            "4  198779           koala-13b   gpt-3.5-turbo-0314   \n",
            "\n",
            "                                              prompt  \\\n",
            "0  [\"Is it morally right to try to have a certain...   \n",
            "1  [\"What is the difference between marriage lice...   \n",
            "2  [\"explain function calling. how would you call...   \n",
            "3  [\"How can I create a test set for a very rare ...   \n",
            "4  [\"What is the best way to travel from Tel-Aviv...   \n",
            "\n",
            "                                          response_a  \\\n",
            "0  [\"The question of whether it is morally right ...   \n",
            "1  [\"A marriage license is a legal document that ...   \n",
            "2  [\"Function calling is the process of invoking ...   \n",
            "3  [\"Creating a test set for a very rare category...   \n",
            "4  [\"The best way to travel from Tel Aviv to Jeru...   \n",
            "\n",
            "                                          response_b  winner_model_a  \\\n",
            "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
            "1  [\"A marriage license and a marriage certificat...               0   \n",
            "2  [\"Function calling is the process of invoking ...               0   \n",
            "3  [\"When building a classifier for a very rare c...               1   \n",
            "4  [\"The best way to travel from Tel-Aviv to Jeru...               0   \n",
            "\n",
            "   winner_model_b  winner_tie  \n",
            "0               0           0  \n",
            "1               1           0  \n",
            "2               0           1  \n",
            "3               0           0  \n",
            "4               1           0  \n",
            "\n",
            "Test data head:\n",
            "        id                                             prompt  \\\n",
            "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
            "1   211333  [\"You are a mediator in a heated political deb...   \n",
            "2  1233961  [\"How to initialize the classification head wh...   \n",
            "\n",
            "                                          response_a  \\\n",
            "0                    [\"You have two oranges today.\"]   \n",
            "1  [\"Thank you for sharing the details of the sit...   \n",
            "2  [\"When you want to initialize the classificati...   \n",
            "\n",
            "                                          response_b  \n",
            "0  [\"You still have three oranges. Eating an oran...  \n",
            "1  [\"Mr Reddy and Ms Blue both have valid points ...  \n",
            "2  [\"To initialize the classification head when p...  \n",
            "\n",
            "Sample submission head:\n",
            "        id  winner_model_a  winner_model_b  winner_tie\n",
            "0   136060        0.333333        0.333333    0.333333\n",
            "1   211333        0.333333        0.333333    0.333333\n",
            "2  1233961        0.333333        0.333333    0.333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Data Preprocessing and Feature Engineering\n",
        "\n",
        "# Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n",
        "    # Optional: Remove stopwords\n",
        "    # stop_words = set(stopwords.words('english'))\n",
        "    # text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to prompt and responses\n",
        "train_df['prompt_cleaned'] = train_df['prompt'].apply(clean_text)\n",
        "train_df['response_a_cleaned'] = train_df['response_a'].apply(clean_text)\n",
        "train_df['response_b_cleaned'] = train_df['response_b'].apply(clean_text)\n",
        "\n",
        "test_df['prompt_cleaned'] = test_df['prompt'].apply(clean_text)\n",
        "test_df['response_a_cleaned'] = test_df['response_a'].apply(clean_text)\n",
        "test_df['response_b_cleaned'] = test_df['response_b'].apply(clean_text)\n",
        "\n",
        "print(\"\\nText cleaning applied.\")\n",
        "print(\"\\nTrain data with cleaned text:\")\n",
        "print(train_df[['prompt_cleaned', 'response_a_cleaned', 'response_b_cleaned']].head())\n",
        "\n",
        "# Tokenization and Padding\n",
        "max_words = 20000 # Maximum number of words to keep based on word frequency\n",
        "maxlen = 256 # Maximum length of sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "\n",
        "# Fit tokenizer on combined text from train and test for better vocabulary coverage\n",
        "all_text = pd.concat([\n",
        "    train_df['prompt_cleaned'], train_df['response_a_cleaned'], train_df['response_b_cleaned'],\n",
        "    test_df['prompt_cleaned'], test_df['response_a_cleaned'], test_df['response_b_cleaned']\n",
        "])\n",
        "tokenizer.fit_on_texts(all_text)\n",
        "\n",
        "# Convert text to sequences\n",
        "train_sequences_prompt = tokenizer.texts_to_sequences(train_df['prompt_cleaned'])\n",
        "train_sequences_response_a = tokenizer.texts_to_sequences(train_df['response_a_cleaned'])\n",
        "train_sequences_response_b = tokenizer.texts_to_sequences(train_df['response_b_cleaned'])\n",
        "\n",
        "test_sequences_prompt = tokenizer.texts_to_sequences(test_df['prompt_cleaned'])\n",
        "test_sequences_response_a = tokenizer.texts_to_sequences(test_df['response_a_cleaned'])\n",
        "test_sequences_response_b = tokenizer.texts_to_sequences(test_df['response_b_cleaned'])\n",
        "\n",
        "# Pad sequences\n",
        "train_padded_prompt = pad_sequences(train_sequences_prompt, maxlen=maxlen, padding='post', truncating='post')\n",
        "train_padded_response_a = pad_sequences(train_sequences_response_a, maxlen=maxlen, padding='post', truncating='post')\n",
        "train_padded_response_b = pad_sequences(train_sequences_response_b, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "test_padded_prompt = pad_sequences(test_sequences_prompt, maxlen=maxlen, padding='post', truncating='post')\n",
        "test_padded_response_a = pad_sequences(test_sequences_response_a, maxlen=maxlen, padding='post', truncating='post')\n",
        "test_padded_response_b = pad_sequences(test_sequences_response_b, maxlen=maxlen, padding='post', truncating='post')\n",
        "\n",
        "print(\"\\nText tokenization and padding applied.\")\n",
        "print(\"\\nExample padded sequence (prompt):\")\n",
        "print(train_padded_prompt[0])\n",
        "\n",
        "# Prepare Target Variable\n",
        "# We need to represent the winner as a one-hot encoded vector\n",
        "# [1, 0, 0] for winner_model_a, [0, 1, 0] for winner_model_b, [0, 0, 1] for winner_tie\n",
        "y_train = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values\n",
        "\n",
        "print(\"\\nTarget variable prepared:\")\n",
        "print(y_train[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vMkiT5EmgME",
        "outputId": "fe6586ae-f9f4-4870-9ace-1f8d2aed4471"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Text cleaning applied.\n",
            "\n",
            "Train data with cleaned text:\n",
            "                                      prompt_cleaned  \\\n",
            "0  is it morally right to try to have a certain p...   \n",
            "1  what is the difference between marriage licens...   \n",
            "2  explain function calling how would you call a ...   \n",
            "3  how can i create a test set for a very rare ca...   \n",
            "4  what is the best way to travel from telaviv to...   \n",
            "\n",
            "                                  response_a_cleaned  \\\n",
            "0  the question of whether it is morally right to...   \n",
            "1  a marriage license is a legal document that al...   \n",
            "2  function calling is the process of invoking or...   \n",
            "3  creating a test set for a very rare category c...   \n",
            "4  the best way to travel from tel aviv to jerusa...   \n",
            "\n",
            "                                  response_b_cleaned  \n",
            "0  as an ai i dont have personal beliefs or opini...  \n",
            "1  a marriage license and a marriage certificate ...  \n",
            "2  function calling is the process of invoking a ...  \n",
            "3  when building a classifier for a very rare cat...  \n",
            "4  the best way to travel from telaviv to jerusal...  \n",
            "\n",
            "Text tokenization and padding applied.\n",
            "\n",
            "Example padded sequence (prompt):\n",
            "[   8   16 6455  246    4  297    4   23    5  300 2178    6 7774   18\n",
            "    1    1  158 7911 5504   18    5 2900 3515    3  260   76  959  199\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0]\n",
            "\n",
            "Target variable prepared:\n",
            "[[1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19f77ee2",
        "outputId": "e7335ecb-5519-4dfa-87d3-ca25523a842f"
      },
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"Pre-trained BERT tokenizer and model loaded successfully.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained BERT tokenizer and model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffcc172a"
      },
      "source": [
        "## Generate embeddings\n",
        "\n",
        "### Subtask:\n",
        "Use the pre-trained model to generate embeddings for the prompt and responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6d4e77c"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to generate BERT embeddings for text inputs using the loaded tokenizer and model, then apply it to the relevant columns in the training and test dataframes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd9c4adc",
        "outputId": "56ca7860-5603-4145-afb4-84495193d727"
      },
      "source": [
        "import torch\n",
        "\n",
        "def generate_bert_embeddings(texts, tokenizer, model, max_len=512, batch_size=32):\n",
        "    \"\"\"Generates BERT embeddings for a list of text inputs in batches.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        encoded_inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            return_tensors='pt'  # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        encoded_inputs = {key: val.to(device) for key, val in encoded_inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_inputs)\n",
        "\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "        embeddings.append(cls_embeddings.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(embeddings)\n",
        "\n",
        "# Generate embeddings for training data\n",
        "print(\"Generating embeddings for training data...\")\n",
        "train_prompt_embeddings = generate_bert_embeddings(train_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_a_embeddings = generate_bert_embeddings(train_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_b_embeddings = generate_bert_embeddings(train_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Training embeddings generated.\")\n",
        "\n",
        "# Generate embeddings for test data\n",
        "print(\"Generating embeddings for test data...\")\n",
        "test_prompt_embeddings = generate_bert_embeddings(test_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_a_embeddings = generate_bert_embeddings(test_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_b_embeddings = generate_bert_embeddings(test_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Test embeddings generated.\")\n",
        "\n",
        "print(\"\\nShape of training prompt embeddings:\", train_prompt_embeddings.shape)\n",
        "print(\"Shape of training response_a embeddings:\", train_response_a_embeddings.shape)\n",
        "print(\"Shape of training response_b embeddings:\", train_response_b_embeddings.shape)\n",
        "print(\"\\nShape of test prompt embeddings:\", test_prompt_embeddings.shape)\n",
        "print(\"Shape of test response_a embeddings:\", test_response_a_embeddings.shape)\n",
        "print(\"Shape of test response_b embeddings:\", test_response_b_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for training data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55166e45"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the dataframes `train_df` and `test_df` were not defined in the current session. Need to reload the dataframes and then regenerate the BERT embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAh9DfrdvNA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure train.csv, test.csv, and sample_submission.csv are in your Colab environment.\")\n",
        "\n",
        "\n",
        "# Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to prompt and responses\n",
        "train_df['prompt_cleaned'] = train_df['prompt'].apply(clean_text)\n",
        "train_df['response_a_cleaned'] = train_df['response_a'].apply(clean_text)\n",
        "train_df['response_b_cleaned'] = train_df['response_b'].apply(clean_text)\n",
        "\n",
        "test_df['prompt_cleaned'] = test_df['prompt'].apply(clean_text)\n",
        "test_df['response_a_cleaned'] = test_df['response_a'].apply(clean_text)\n",
        "test_df['response_b_cleaned'] = test_df['response_b'].apply(clean_text)\n",
        "\n",
        "print(\"\\nText cleaning applied.\")\n",
        "\n",
        "# Load a pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "print(\"\\nPre-trained BERT tokenizer and model loaded successfully.\")\n",
        "\n",
        "def generate_bert_embeddings(texts, tokenizer, model, max_len=512):\n",
        "    \"\"\"Generates BERT embeddings for a list of text inputs.\"\"\"\n",
        "    # Tokenize texts\n",
        "    encoded_inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='pt'  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Move inputs to the same device as the model (if using GPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    encoded_inputs = {key: val.to(device) for key, val in encoded_inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    # Get hidden states from BERT\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(**encoded_inputs)\n",
        "\n",
        "    # Aggregate hidden states (using the embedding of the [CLS] token)\n",
        "    # The output 'last_hidden_state' has shape (batch_size, sequence_length, hidden_size)\n",
        "    # The [CLS] token is at the beginning (index 0) of the sequence\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    return cls_embeddings.cpu().numpy() # Move back to CPU and convert to numpy\n",
        "\n",
        "# Generate embeddings for training data\n",
        "print(\"Generating embeddings for training data...\")\n",
        "train_prompt_embeddings = generate_bert_embeddings(train_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_a_embeddings = generate_bert_embeddings(train_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_b_embeddings = generate_bert_embeddings(train_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Training embeddings generated.\")\n",
        "\n",
        "# Generate embeddings for test data\n",
        "print(\"Generating embeddings for test data...\")\n",
        "test_prompt_embeddings = generate_bert_embeddings(test_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_a_embeddings = generate_bert_embeddings(test_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_b_embeddings = generate_bert_embeddings(test_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Test embeddings generated.\")\n",
        "\n",
        "print(\"\\nShape of training prompt embeddings:\", train_prompt_embeddings.shape)\n",
        "print(\"Shape of training response_a embeddings:\", train_response_a_embeddings.shape)\n",
        "print(\"Shape of training response_b embeddings:\", train_response_b_embeddings.shape)\n",
        "print(\"\\nShape of test prompt embeddings:\", test_prompt_embeddings.shape)\n",
        "print(\"Shape of test response_a embeddings:\", test_response_a_embeddings.shape)\n",
        "print(\"Shape of test response_b embeddings:\", test_response_b_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "103b39c3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `stopwords` was not imported from `nltk`. Need to import `stopwords` and then retry the data loading, cleaning, and embedding generation process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb05jPSohPUs"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure train.csv, test.csv, and sample_submission.csv are in your Colab environment.\")\n",
        "\n",
        "\n",
        "# Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to prompt and responses\n",
        "train_df['prompt_cleaned'] = train_df['prompt'].apply(clean_text)\n",
        "train_df['response_a_cleaned'] = train_df['response_a'].apply(clean_text)\n",
        "train_df['response_b_cleaned'] = train_df['response_b'].apply(clean_text)\n",
        "\n",
        "test_df['prompt_cleaned'] = test_df['prompt'].apply(clean_text)\n",
        "test_df['response_a_cleaned'] = test_df['response_a'].apply(clean_text)\n",
        "test_df['response_b_cleaned'] = test_df['response_b'].apply(clean_text)\n",
        "\n",
        "print(\"\\nText cleaning applied.\")\n",
        "\n",
        "# Load a pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "print(\"\\nPre-trained BERT tokenizer and model loaded successfully.\")\n",
        "\n",
        "def generate_bert_embeddings(texts, tokenizer, model, max_len=512):\n",
        "    \"\"\"Generates BERT embeddings for a list of text inputs.\"\"\"\n",
        "    # Tokenize texts\n",
        "    encoded_inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='pt'  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Move inputs to the same device as the model (if using GPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    encoded_inputs = {key: val.to(device) for key, val in encoded_inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    # Get hidden states from BERT\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(**encoded_inputs)\n",
        "\n",
        "    # Aggregate hidden states (using the embedding of the [CLS] token)\n",
        "    # The output 'last_hidden_state' has shape (batch_size, sequence_length, hidden_size)\n",
        "    # The [CLS] token is at the beginning (index 0) of the sequence\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    return cls_embeddings.cpu().numpy() # Move back to CPU and convert to numpy\n",
        "\n",
        "# Generate embeddings for training data\n",
        "print(\"Generating embeddings for training data...\")\n",
        "train_prompt_embeddings = generate_bert_embeddings(train_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_a_embeddings = generate_bert_embeddings(train_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_b_embeddings = generate_bert_embeddings(train_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Training embeddings generated.\")\n",
        "\n",
        "# Generate embeddings for test data\n",
        "print(\"Generating embeddings for test data...\")\n",
        "test_prompt_embeddings = generate_bert_embeddings(test_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_a_embeddings = generate_bert_embeddings(test_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_b_embeddings = generate_bert_embeddings(test_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Test embeddings generated.\")\n",
        "\n",
        "print(\"\\nShape of training prompt embeddings:\", train_prompt_embeddings.shape)\n",
        "print(\"Shape of training response_a embeddings:\", train_response_a_embeddings.shape)\n",
        "print(\"Shape of training response_b embeddings:\", train_response_b_embeddings.shape)\n",
        "print(\"\\nShape of test prompt embeddings:\", test_prompt_embeddings.shape)\n",
        "print(\"Shape of test response_a embeddings:\", test_response_a_embeddings.shape)\n",
        "print(\"Shape of test response_b embeddings:\", test_response_b_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2244f462"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the dataframes were not loaded successfully within the `try...except` block due to a `FileNotFoundError` message being printed, but the program continued trying to access the dataframes. The code needs to ensure the dataframes are loaded before attempting to clean and process them. The previous data loading cell succeeded, indicating the files are present. I will remove the try-except block and directly load the files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvu0asFYhhG7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Load the datasets - Direct load as previous successful execution confirms files are present\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "sample_submission_df = pd.read_csv('sample_submission.csv')\n",
        "print(\"Data loaded successfully!\")\n",
        "\n",
        "\n",
        "# Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to prompt and responses\n",
        "train_df['prompt_cleaned'] = train_df['prompt'].apply(clean_text)\n",
        "train_df['response_a_cleaned'] = train_df['response_a'].apply(clean_text)\n",
        "train_df['response_b_cleaned'] = train_df['response_b'].apply(clean_text)\n",
        "\n",
        "test_df['prompt_cleaned'] = test_df['prompt'].apply(clean_text)\n",
        "test_df['response_a_cleaned'] = test_df['response_a'].apply(clean_text)\n",
        "test_df['response_b_cleaned'] = test_df['response_b'].apply(clean_text)\n",
        "\n",
        "print(\"\\nText cleaning applied.\")\n",
        "\n",
        "# Load a pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "print(\"\\nPre-trained BERT tokenizer and model loaded successfully.\")\n",
        "\n",
        "def generate_bert_embeddings(texts, tokenizer, model, max_len=512):\n",
        "    \"\"\"Generates BERT embeddings for a list of text inputs.\"\"\"\n",
        "    # Tokenize texts\n",
        "    encoded_inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='pt'  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Move inputs to the same device as the model (if using GPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    encoded_inputs = {key: val.to(device) for key, val in encoded_inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    # Get hidden states from BERT\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(**encoded_inputs)\n",
        "\n",
        "    # Aggregate hidden states (using the embedding of the [CLS] token)\n",
        "    # The output 'last_hidden_state' has shape (batch_size, sequence_length, hidden_size)\n",
        "    # The [CLS] token is at the beginning (index 0) of the sequence\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    return cls_embeddings.cpu().numpy() # Move back to CPU and convert to numpy\n",
        "\n",
        "# Generate embeddings for training data\n",
        "print(\"Generating embeddings for training data...\")\n",
        "train_prompt_embeddings = generate_bert_embeddings(train_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_a_embeddings = generate_bert_embeddings(train_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_b_embeddings = generate_bert_embeddings(train_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Training embeddings generated.\")\n",
        "\n",
        "# Generate embeddings for test data\n",
        "print(\"Generating embeddings for test data...\")\n",
        "test_prompt_embeddings = generate_bert_embeddings(test_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_a_embeddings = generate_bert_embeddings(test_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_b_embeddings = generate_bert_embeddings(test_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Test embeddings generated.\")\n",
        "\n",
        "print(\"\\nShape of training prompt embeddings:\", train_prompt_embeddings.shape)\n",
        "print(\"Shape of training response_a embeddings:\", train_response_a_embeddings.shape)\n",
        "print(\"Shape of training response_b embeddings:\", train_response_b_embeddings.shape)\n",
        "print(\"\\nShape of test prompt embeddings:\", test_prompt_embeddings.shape)\n",
        "print(\"Shape of test response_a embeddings:\", test_response_a_embeddings.shape)\n",
        "print(\"Shape of test response_b embeddings:\", test_response_b_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d6869da"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the files were not found. Although a previous cell indicated successful unzipping, the current environment seems unable to locate the files directly. I will try loading the files from the '/content/' directory which is the default location after unzipping in Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "338b923c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the dataframes `train_df` and `test_df` were not defined in the current session. Need to reload the dataframes and then regenerate the BERT embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36a7d7c9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "# Download NLTK stopwords (if not already downloaded)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sample_submission_df = pd.read_csv('sample_submission.csv')\n",
        "    print(\"Data loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Ensure train.csv, test.csv, and sample_submission.csv are in your Colab environment.\")\n",
        "\n",
        "\n",
        "# Text Cleaning Function\n",
        "def clean_text(text):\n",
        "    text = text.lower() # Convert to lowercase\n",
        "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "# Apply cleaning to prompt and responses\n",
        "train_df['prompt_cleaned'] = train_df['prompt'].apply(clean_text)\n",
        "train_df['response_a_cleaned'] = train_df['response_a'].apply(clean_text)\n",
        "train_df['response_b_cleaned'] = train_df['response_b'].apply(clean_text)\n",
        "\n",
        "test_df['prompt_cleaned'] = test_df['prompt'].apply(clean_text)\n",
        "test_df['response_a_cleaned'] = test_df['response_a'].apply(clean_text)\n",
        "test_df['response_b_cleaned'] = test_df['response_b'].apply(clean_text)\n",
        "\n",
        "print(\"\\nText cleaning applied.\")\n",
        "\n",
        "# Load a pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "print(\"\\nPre-trained BERT tokenizer and model loaded successfully.\")\n",
        "\n",
        "def generate_bert_embeddings(texts, tokenizer, model, max_len=512):\n",
        "    \"\"\"Generates BERT embeddings for a list of text inputs.\"\"\"\n",
        "    # Tokenize texts\n",
        "    encoded_inputs = tokenizer(\n",
        "        texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='pt'  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    # Move inputs to the same device as the model (if using GPU)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    encoded_inputs = {key: val.to(device) for key, val in encoded_inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    # Get hidden states from BERT\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        outputs = model(**encoded_inputs)\n",
        "\n",
        "    # Aggregate hidden states (using the embedding of the [CLS] token)\n",
        "    # The output 'last_hidden_state' has shape (batch_size, sequence_length, hidden_size)\n",
        "    # The [CLS] token is at the beginning (index 0) of the sequence\n",
        "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    return cls_embeddings.cpu().numpy() # Move back to CPU and convert to numpy\n",
        "\n",
        "# Generate embeddings for training data\n",
        "print(\"Generating embeddings for training data...\")\n",
        "train_prompt_embeddings = generate_bert_embeddings(train_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_a_embeddings = generate_bert_embeddings(train_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "train_response_b_embeddings = generate_bert_embeddings(train_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Training embeddings generated.\")\n",
        "\n",
        "# Generate embeddings for test data\n",
        "print(\"Generating embeddings for test data...\")\n",
        "test_prompt_embeddings = generate_bert_embeddings(test_df['prompt_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_a_embeddings = generate_bert_embeddings(test_df['response_a_cleaned'].tolist(), tokenizer, model)\n",
        "test_response_b_embeddings = generate_bert_embeddings(test_df['response_b_cleaned'].tolist(), tokenizer, model)\n",
        "print(\"Test embeddings generated.\")\n",
        "\n",
        "print(\"\\nShape of training prompt embeddings:\", train_prompt_embeddings.shape)\n",
        "print(\"Shape of training response_a embeddings:\", train_response_a_embeddings.shape)\n",
        "print(\"Shape of training response_b embeddings:\", train_response_b_embeddings.shape)\n",
        "print(\"\\nShape of test prompt embeddings:\", test_prompt_embeddings.shape)\n",
        "print(\"Shape of test response_a embeddings:\", test_response_a_embeddings.shape)\n",
        "print(\"Shape of test response_b embeddings:\", test_response_b_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}